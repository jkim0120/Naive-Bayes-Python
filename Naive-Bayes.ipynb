{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes Algorithm\n",
    "=====================\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is It?\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes algorithm is an intuitive approach to making predictions based on prior beliefs or probabilities. Quoting Jason Brownlee, \"it is the supervised learning approach you would come up with if you wanted to model a predictive modeling problem probabilistically\".\n",
    "\n",
    "Let's dive into the mathematics. We start off with a belief or a *prior probability* of event $A$. This is denoted as $P(A)$. Now, everything seems to be going well until we're hit with some new evidence $X$, which implies something that affects the probability of our belief. As much as we'd like to, we can't simply ignore $X$ and go home. Instead, given evidence $X$, we must calculate a new value for event $A$ called the *posterior probability*. This is denoted as $P(A | X)$. Finally, for the sake of completion, $P(X | A)$ is the probability of observing evidence $X$ for event $A$ and $P(X)$ is the untouched probability of observing evidence $X$.\n",
    "\n",
    "\\begin{align}\n",
    " P( A | X ) = & \\frac{ P(X | A) P(A) } {P(X) } \\\\\\\\[5pt]\n",
    "\\end{align}\n",
    "\n",
    "You're probably wondering what makes this algorithm *naive*. Well, it's due to the underlying assumption that the probability of event $A$ given any evidence $X_n$ is totally independent of each other. This simplifies a lot of things and explains its popularity in many fields.\n",
    "\n",
    "The content of this notebook uses Python to classify whether a patient is diagnosed with diabetes given a set of attributes. The data set is called the \"Pima Indians Diabetes Data Set\" provided by the National Institute of Diabetes and Digestive and Kidney Diseases. The target accuracy to indicate the algorithm's credibility is between 70% - 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Formatting\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is given as a `csv` file, which requires parsing and partitioning to form a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from pima-indians-diabetes.data.csv with 768 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def load_csv(file):\n",
    "    lines = csv.reader(open(file, 'rb'))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [float(x) for x in dataset[i]]\n",
    "    return dataset\n",
    "\n",
    "file = \"pima-indians-diabetes.data.csv\"\n",
    "dataset = load_csv(file)\n",
    "print('Loaded data from {0} with {1} rows').format(file, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split total data (768 rows) into training set (514 rows) and testing set (254 rows)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "def partition_data(dataset, ratio):\n",
    "    train_size = int(len(dataset) * ratio)\n",
    "    test_set = list(dataset)\n",
    "    train_set = []\n",
    "    \n",
    "    while len(train_set) < train_size:\n",
    "        index = randrange(len(test_set))\n",
    "        train_set.append(test_set.pop(index))\n",
    "        \n",
    "    return [train_set, test_set]\n",
    "\n",
    "train_set, test_set = partition_data(dataset)\n",
    "print('Split total data ({0} rows) into training set ({1} rows) and testing set ({2} rows)').format(len(dataset), len(train_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Organization and Pre-calculations\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been partitioned, let's visualize what it actually looks like and discuss how we should transform it going forward. Currently, our training set, $T$, can be described as an $m \\times n$ matrix,\n",
    "\n",
    "\\begin{align}\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where $m$ is the number of data points and $n$ is the number of attributes plus the classification value for each data point.\n",
    "\n",
    "Next, we want to group our data points by class, $T(0)$ and $T(1)$,\n",
    "\n",
    "\\begin{align}\n",
    "T(0) = \n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots & x_{1n-1} & 0 \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots & x_{2n-1} & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots & x_{mn-1} & 0\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "T(1) = \n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots & x_{1n-1} & 1 \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots & x_{2n-1} & 1 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots & x_{mn-1} & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In the implementation, we'll organize each datapoint into a map where the key is the classification value and its values are the data points that belong to it. The classification value for each data point will be removed afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def group_by_class(dataset):\n",
    "    klass_map = {}\n",
    "    for el in dataset:\n",
    "        klass = int(el[-1])\n",
    "        if klass not in klass_map:\n",
    "            klass_map[klass] = []\n",
    "        klass_map[klass].append(el[:-1])\n",
    "    return klass_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each attribute, we want to calculate the mean and standard deviation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
