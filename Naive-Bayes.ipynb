{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes Algorithm\n",
    "=====================\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is It?\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes algorithm is an intuitive approach to making predictions based on prior beliefs or probabilities. Quoting Jason Brownlee, \"it is the supervised learning approach you would come up with if you wanted to model a predictive modeling problem probabilistically\".\n",
    "\n",
    "Let's dive into the mathematics. We start off with a belief or a *prior probability* of event $A$. This is denoted as $P(A)$. Now, everything seems to be going well until we're hit with some new evidence $X$, which implies something that affects the probability of our belief. As much as we'd like to, we can't simply ignore $X$ and go home. Instead, given evidence $X$, we must calculate a new value for event $A$ called the *posterior probability*. This is denoted as $P(A | X)$. Finally, for the sake of completion, $P(X | A)$ is the probability of observing evidence $X$ for event $A$ and $P(X)$ is the untouched probability of observing evidence $X$.\n",
    "\n",
    "\\begin{align}\n",
    " P( A | X ) = & \\frac{ P(X | A) P(A) } {P(X) } \\\\\\\\[5pt]\n",
    "\\end{align}\n",
    "\n",
    "You're probably wondering what makes this algorithm *naive*. Well, it's due to the underlying assumption that the probability of event $A$ given any evidence $X_n$ is totally independent of each other. This simplifies a lot of things and explains its popularity in many fields.\n",
    "\n",
    "The content of this notebook uses Python to classify whether a patient is diagnosed with diabetes given a set of attributes. The data set is called the \"Pima Indians Diabetes Data Set\" provided by the National Institute of Diabetes and Digestive and Kidney Diseases. The target accuracy to indicate the algorithm's credibility is between 70% - 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Formatting\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is given as a `csv` file, which requires parsing and partitioning to form a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from pima-indians-diabetes.data.csv with 768 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def load_csv(file):\n",
    "    lines = csv.reader(open(file, 'rb'))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [float(x) for x in dataset[i]]\n",
    "    return dataset\n",
    "\n",
    "file = \"pima-indians-diabetes.data.csv\"\n",
    "dataset = load_csv(file)\n",
    "print('Loaded data from {0} with {1} rows').format(file, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split total data (768 rows) into training set (514 rows) and testing set (254 rows)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "def partition_data(dataset, ratio):\n",
    "    train_size = int(len(dataset) * ratio)\n",
    "    test_set = list(dataset)\n",
    "    train_set = []\n",
    "    \n",
    "    while len(train_set) < train_size:\n",
    "        index = randrange(len(test_set))\n",
    "        train_set.append(test_set.pop(index))\n",
    "        \n",
    "    return [train_set, test_set]\n",
    "\n",
    "train_set, test_set = partition_data(dataset, 0.67)\n",
    "print('Split total data ({0} rows) into training set ({1} rows) and testing set ({2} rows)').format(len(dataset), len(train_set), len(test_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Organization and Pre-calculations\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been partitioned, let's visualize what it actually looks like and discuss how we should transform it going forward. Currently, our training set, $T$, can be described as an $m \\times n$ matrix,\n",
    "\n",
    "\\begin{align}\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where $m$ is the number of data points and $n$ is the number of attributes including the classification value for each data point.\n",
    "\n",
    "Next, we want to summarize our data. First, we want to group our data points by classification value, giving us two matrices, $T(0)$ and $T(1)$. Afterwards, we must calculate the mean and standard devation for each attribute and store them as a matrix of tuples in preparation for our probability function later on.\n",
    "\n",
    "\\begin{align}\n",
    "T(0) = \n",
    "\\begin{bmatrix}\n",
    "    (\\bar{x}, s)_{1} \\\\\n",
    "    (\\bar{x}, s)_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    (\\bar{x}, s)_{n}\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\\\\n",
    "T(1) = \n",
    "\\begin{bmatrix}\n",
    "    (\\bar{x}, s)_{1} \\\\\n",
    "    (\\bar{x}, s)_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    (\\bar{x}, s)_{n}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In the implementation, we'll summarize the data set into a dict where each key and value are the classification value and attribute summaries, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 contains 325 data points\n",
      "Class 1 contains 189 data points\n"
     ]
    }
   ],
   "source": [
    "def group_by_class(dataset):\n",
    "    klass_map = {}\n",
    "    for el in dataset:\n",
    "        klass = int(el[-1])\n",
    "        if klass not in klass_map:\n",
    "            klass_map[klass] = []\n",
    "        klass_map[klass].append(el[:-1])\n",
    "    return klass_map\n",
    "\n",
    "classified_set = group_by_class(train_set)\n",
    "\n",
    "for klass, data_points in classified_set.iteritems():\n",
    "    print('Class {0} contains {1} data points').format(klass, len(data_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our mean and standard deviation calculations as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def mean(n):\n",
    "    return sum(n) / float(len(n))\n",
    "\n",
    "def stdev(n):\n",
    "    average = mean(n)\n",
    "    return math.sqrt(sum([pow(x - average, 2) for x in n]) / float(len(n) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize our process time by parallelizing our mean and standard deviation calculations using the built-in multiprocessor library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 contains 8 tuples\n",
      "Class 1 contains 8 tuples\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def format_calc(t):\n",
    "    return (mean(t), stdev(t))\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    summary = {}\n",
    "    for klass, data_points in dataset.iteritems():\n",
    "        summary[klass] = pool.map(format_calc, zip(*data_points))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return summary\n",
    "\n",
    "summary_set = prepare_data(classified_set)\n",
    "\n",
    "for klass, tupl in summary_set.iteritems():\n",
    "    print('Class {0} contains {1} tuples').format(klass, len(tupl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Training and Testing\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the Naive-Bayes classifier using our training set and a Guassian probability function to produce $P(A|X)$. This is the probability of an attribute belonging to a class and due to the assumption that each probability is independent from each other, the calculation process can be parallelized (for future implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gauss(x, mean, stdev):\n",
    "    ex = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2))))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * stdev)) * ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(summary_set, data_point):\n",
    "    probabilities = {}\n",
    "    for klass, summary in summary_set.iteritems():\n",
    "        probabilities[klass] = 1\n",
    "        for i in xrange(len(summary)):\n",
    "            mean, stdev = summary[i]\n",
    "            probabilities[klass] *= gauss(data_point[i], mean, stdev)\n",
    "    return max(probabilities.iterkeys(), key=(lambda key: probabilities[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model against the training set and hope that the accuracy falls in the acceptable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive-Bayes Model yields 75.59% accuracy\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(summary_set, test_set):\n",
    "    correct_count = 0\n",
    "    for test_point in test_set:\n",
    "        if test_point[-1] == predict(summary_set, test_point):\n",
    "            correct_count += 1\n",
    "    return correct_count / float(len(test_set)) * 100\n",
    "\n",
    "accuracy = get_accuracy(summary_set, test_set)\n",
    "\n",
    "print('The Naive-Bayes Model yields {0}% accuracy').format(round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy lies between 70% - 76% range; therefore, our model is accurate (at least for the purpose of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
